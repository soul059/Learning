# 02. Mathematics for Machine Learning

## ğŸ¯ Learning Objectives
- Master essential linear algebra concepts for ML
- Understand calculus applications in optimization
- Learn probability and statistics fundamentals
- Apply mathematical concepts to ML algorithms

---

## 1. Linear Algebra Foundations

Linear algebra is the backbone of machine learning, providing the mathematical framework for data representation and algorithm implementation.

### 1.1 Vectors ğŸŸ¢

**Definition**: A vector is an ordered list of numbers representing magnitude and direction.

#### Vector Notation:
```
Column Vector: v = [vâ‚]    Row Vector: v = [vâ‚, vâ‚‚, vâ‚ƒ]
                  [vâ‚‚]
                  [vâ‚ƒ]
```

#### Vector Operations:

**Addition/Subtraction:**
```
a = [1, 2, 3]
b = [4, 5, 6]
a + b = [5, 7, 9]
a - b = [-3, -3, -3]
```

**Scalar Multiplication:**
```
c = 3
c * a = [3, 6, 9]
```

**Dot Product (Inner Product):**
```
a Â· b = aâ‚bâ‚ + aâ‚‚bâ‚‚ + aâ‚ƒbâ‚ƒ = 1Ã—4 + 2Ã—5 + 3Ã—6 = 32
```

**Magnitude (Euclidean Norm):**
```
||a|| = âˆš(aâ‚Â² + aâ‚‚Â² + aâ‚ƒÂ²) = âˆš(1Â² + 2Â² + 3Â²) = âˆš14
```

#### ML Applications:
- **Feature vectors**: Representing data points
- **Weight vectors**: Model parameters
- **Similarity measurement**: Cosine similarity using dot product

### 1.2 Matrices ğŸŸ¢

**Definition**: A rectangular array of numbers arranged in rows and columns.

#### Matrix Notation:
```
A = [aâ‚â‚  aâ‚â‚‚  aâ‚â‚ƒ]  (m Ã— n matrix: m rows, n columns)
    [aâ‚‚â‚  aâ‚‚â‚‚  aâ‚‚â‚ƒ]
```

#### Matrix Operations:

**Addition/Subtraction:**
```
A + B = [aâ‚â‚+bâ‚â‚  aâ‚â‚‚+bâ‚â‚‚]
        [aâ‚‚â‚+bâ‚‚â‚  aâ‚‚â‚‚+bâ‚‚â‚‚]
```

**Matrix Multiplication:**
```
C = A Ã— B where C[i,j] = Î£(A[i,k] Ã— B[k,j])

Example:
[1  2] Ã— [5  6] = [1Ã—5+2Ã—7  1Ã—6+2Ã—8] = [19  22]
[3  4]   [7  8]   [3Ã—5+4Ã—7  3Ã—6+4Ã—8]   [43  50]
```

**Transpose:**
```
A = [1  2  3]  â†’  Aáµ€ = [1  4]
    [4  5  6]          [2  5]
                       [3  6]
```

**Identity Matrix:**
```
I = [1  0  0]  (AI = IA = A)
    [0  1  0]
    [0  0  1]
```

**Inverse Matrix:**
```
Aâ»Â¹ such that AAâ»Â¹ = Aâ»Â¹A = I
```

#### Special Matrices:

**Symmetric Matrix:** A = Aáµ€
**Orthogonal Matrix:** AAáµ€ = I
**Diagonal Matrix:** Non-zero elements only on diagonal

#### ML Applications:
- **Data matrices**: Rows = samples, columns = features
- **Transformation matrices**: Linear transformations
- **Covariance matrices**: Feature relationships

### 1.3 Eigenvalues and Eigenvectors ğŸŸ¡

**Definition**: For matrix A, vector v is an eigenvector with eigenvalue Î» if:
```
Av = Î»v
```

#### Key Properties:
- Eigenvectors show directions of maximum variance
- Eigenvalues indicate the magnitude of variance
- Used in dimensionality reduction (PCA)

#### Calculation Example:
```
For A = [3  1], find eigenvalues:
        [0  2]

det(A - Î»I) = 0
det([3-Î»  1  ]) = (3-Î»)(2-Î») = 0
   ([0   2-Î»])

Î»â‚ = 3, Î»â‚‚ = 2
```

#### ML Applications:
- **Principal Component Analysis (PCA)**
- **Spectral clustering**
- **Facial recognition** (eigenfaces)

### 1.4 Matrix Decompositions ğŸŸ¡

#### Singular Value Decomposition (SVD):
```
A = UÎ£Váµ€
```
Where:
- U: Left singular vectors
- Î£: Diagonal matrix of singular values
- V: Right singular vectors

#### ML Applications:
- **Dimensionality reduction**
- **Recommendation systems**
- **Image compression**
- **Latent Semantic Analysis**

---

## 2. Calculus for Optimization

Calculus provides the mathematical foundation for optimizing machine learning models.

### 2.1 Derivatives ğŸŸ¢

**Definition**: Rate of change of a function with respect to its variable.

#### Basic Rules:
```
d/dx(c) = 0                    (constant)
d/dx(x^n) = nx^(n-1)          (power rule)
d/dx(e^x) = e^x               (exponential)
d/dx(ln(x)) = 1/x             (logarithm)
d/dx(sin(x)) = cos(x)         (trigonometric)
```

#### Chain Rule:
```
d/dx[f(g(x))] = f'(g(x)) Ã— g'(x)
```

#### ML Applications:
- **Gradient calculation**: Finding steepest ascent/descent
- **Backpropagation**: Training neural networks
- **Optimization**: Minimizing loss functions

### 2.2 Partial Derivatives ğŸŸ¢

**Definition**: Derivative with respect to one variable while keeping others constant.

#### Notation:
```
âˆ‚f/âˆ‚x = partial derivative of f with respect to x
```

#### Example:
```
f(x,y) = xÂ² + 3xy + yÂ²
âˆ‚f/âˆ‚x = 2x + 3y
âˆ‚f/âˆ‚y = 3x + 2y
```

#### Gradient Vector:
```
âˆ‡f = [âˆ‚f/âˆ‚xâ‚]
     [âˆ‚f/âˆ‚xâ‚‚]
     [  â‹®  ]
     [âˆ‚f/âˆ‚xâ‚™]
```

### 2.3 Optimization ğŸŸ¡

#### Critical Points:
Points where âˆ‡f = 0 (all partial derivatives are zero)

#### Types of Critical Points:
- **Global minimum**: Lowest point overall
- **Local minimum**: Lowest in neighborhood
- **Saddle point**: Neither maximum nor minimum

#### Hessian Matrix:
```
H = [âˆ‚Â²f/âˆ‚xâ‚Â²    âˆ‚Â²f/âˆ‚xâ‚âˆ‚xâ‚‚  â‹¯]
    [âˆ‚Â²f/âˆ‚xâ‚‚âˆ‚xâ‚  âˆ‚Â²f/âˆ‚xâ‚‚Â²    â‹¯]
    [     â‹®           â‹®       â‹±]
```

#### Second Derivative Test:
- **Positive definite Hessian**: Local minimum
- **Negative definite Hessian**: Local maximum
- **Indefinite Hessian**: Saddle point

### 2.4 Gradient Descent ğŸŸ¡

**Algorithm**: Iteratively move in direction of steepest descent

```
Î¸(t+1) = Î¸(t) - Î±âˆ‡f(Î¸(t))
```

Where:
- Î¸: Parameters
- Î±: Learning rate
- âˆ‡f: Gradient of cost function

#### Variants:
- **Batch Gradient Descent**: Use entire dataset
- **Stochastic Gradient Descent**: Use one sample at a time
- **Mini-batch Gradient Descent**: Use small batches

#### ML Applications:
- **Linear regression**: Minimizing mean squared error
- **Logistic regression**: Maximizing likelihood
- **Neural networks**: Backpropagation algorithm

---

## 3. Probability Theory

Probability provides the foundation for handling uncertainty in machine learning.

### 3.1 Basic Probability ğŸŸ¢

#### Sample Space and Events:
- **Sample Space (Î©)**: Set of all possible outcomes
- **Event (A)**: Subset of sample space
- **Probability P(A)**: Number between 0 and 1

#### Axioms of Probability:
1. P(A) â‰¥ 0 for any event A
2. P(Î©) = 1
3. P(A âˆª B) = P(A) + P(B) if A and B are disjoint

#### Basic Rules:
```
P(A') = 1 - P(A)                    (complement)
P(A âˆª B) = P(A) + P(B) - P(A âˆ© B)   (union)
```

### 3.2 Conditional Probability ğŸŸ¢

**Definition**: Probability of A given that B has occurred

```
P(A|B) = P(A âˆ© B) / P(B)
```

#### Independence:
Events A and B are independent if:
```
P(A|B) = P(A)  or  P(A âˆ© B) = P(A)P(B)
```

#### Bayes' Theorem:
```
P(A|B) = P(B|A)P(A) / P(B)
```

Where:
- P(A|B): Posterior probability
- P(B|A): Likelihood
- P(A): Prior probability
- P(B): Marginal probability

#### ML Applications:
- **Naive Bayes classifier**
- **Bayesian inference**
- **Medical diagnosis**
- **Spam filtering**

### 3.3 Random Variables ğŸŸ¡

**Definition**: Function that assigns numerical values to outcomes of random experiments.

#### Types:
- **Discrete**: Countable values (coin flips, dice)
- **Continuous**: Uncountable values (height, temperature)

#### Probability Mass Function (PMF):
For discrete random variable X:
```
P(X = x) = probability that X takes value x
```

#### Probability Density Function (PDF):
For continuous random variable X:
```
f(x) such that P(a â‰¤ X â‰¤ b) = âˆ«[a to b] f(x)dx
```

#### Cumulative Distribution Function (CDF):
```
F(x) = P(X â‰¤ x)
```

### 3.4 Important Distributions ğŸŸ¡

#### Bernoulli Distribution:
```
X ~ Bernoulli(p)
P(X = 1) = p, P(X = 0) = 1-p
```

#### Binomial Distribution:
```
X ~ Binomial(n, p)
P(X = k) = C(n,k) Ã— p^k Ã— (1-p)^(n-k)
```

#### Normal (Gaussian) Distribution:
```
X ~ N(Î¼, ÏƒÂ²)
f(x) = (1/âˆš(2Ï€ÏƒÂ²)) Ã— e^(-(x-Î¼)Â²/(2ÏƒÂ²))
```

#### Standard Normal Distribution:
```
Z ~ N(0, 1)
```

#### ML Applications:
- **Gaussian distribution**: Assumption in many algorithms
- **Bernoulli**: Binary classification
- **Multinomial**: Multi-class classification

### 3.5 Expectation and Variance ğŸŸ¡

#### Expectation (Mean):
```
E[X] = Î£ x Ã— P(X = x)  (discrete)
E[X] = âˆ« x Ã— f(x)dx    (continuous)
```

#### Variance:
```
Var(X) = E[(X - E[X])Â²] = E[XÂ²] - (E[X])Â²
```

#### Standard Deviation:
```
Ïƒ = âˆšVar(X)
```

#### Properties:
```
E[aX + b] = aE[X] + b
Var(aX + b) = aÂ²Var(X)
E[X + Y] = E[X] + E[Y]
Var(X + Y) = Var(X) + Var(Y)  (if X,Y independent)
```

---

## 4. Statistics for Machine Learning

Statistics provides tools for analyzing data and making inferences.

### 4.1 Descriptive Statistics ğŸŸ¢

#### Measures of Central Tendency:
- **Mean**: Average value
- **Median**: Middle value when sorted
- **Mode**: Most frequent value

#### Measures of Spread:
- **Range**: Max - Min
- **Variance**: Average squared deviation from mean
- **Standard Deviation**: Square root of variance
- **Interquartile Range (IQR)**: Q3 - Q1

#### Distribution Shape:
- **Skewness**: Asymmetry of distribution
- **Kurtosis**: Tail heaviness

### 4.2 Inferential Statistics ğŸŸ¡

#### Sampling:
- **Population**: Complete set of items
- **Sample**: Subset of population
- **Sampling bias**: Non-representative sample

#### Central Limit Theorem:
Sample means approach normal distribution as sample size increases.

#### Confidence Intervals:
Range likely to contain population parameter with given confidence level.

#### Hypothesis Testing:
- **Null hypothesis (Hâ‚€)**: No effect/difference
- **Alternative hypothesis (Hâ‚)**: Effect exists
- **p-value**: Probability of observing data given Hâ‚€ is true
- **Type I error**: Rejecting true Hâ‚€ (false positive)
- **Type II error**: Accepting false Hâ‚€ (false negative)

### 4.3 Correlation and Regression ğŸŸ¡

#### Correlation:
Measure of linear relationship between variables.

**Pearson Correlation Coefficient:**
```
r = Î£[(xi - xÌ„)(yi - È³)] / âˆš[Î£(xi - xÌ„)Â²Î£(yi - È³)Â²]
```

Values: -1 (perfect negative) to +1 (perfect positive)

#### Simple Linear Regression:
```
y = Î²â‚€ + Î²â‚x + Îµ
```

Where:
- Î²â‚€: Intercept
- Î²â‚: Slope
- Îµ: Error term

#### Least Squares Estimation:
```
Î²â‚ = Î£[(xi - xÌ„)(yi - È³)] / Î£(xi - xÌ„)Â²
Î²â‚€ = È³ - Î²â‚xÌ„
```

### 4.4 Information Theory ğŸŸ¡

#### Entropy:
Measure of uncertainty/information content.

```
H(X) = -Î£ P(xi) logâ‚‚ P(xi)
```

#### Cross-Entropy:
```
H(p,q) = -Î£ p(xi) log q(xi)
```

#### Kullback-Leibler (KL) Divergence:
```
KL(p||q) = Î£ p(xi) log(p(xi)/q(xi))
```

#### Mutual Information:
```
I(X;Y) = Î£ P(x,y) log(P(x,y)/(P(x)P(y)))
```

#### ML Applications:
- **Decision trees**: Information gain
- **Neural networks**: Cross-entropy loss
- **Feature selection**: Mutual information

---

## 5. Mathematical Optimization

Optimization is central to training machine learning models.

### 5.1 Optimization Problems ğŸŸ¡

#### General Form:
```
minimize f(x)
subject to: gi(x) â‰¤ 0, i = 1,...,m
           hj(x) = 0, j = 1,...,p
```

#### Types:
- **Unconstrained**: No constraints on variables
- **Constrained**: Variables must satisfy constraints
- **Convex**: Global minimum exists and is findable
- **Non-convex**: Multiple local minima may exist

### 5.2 Convex Optimization ğŸŸ¡

#### Convex Function:
```
f(Î»x + (1-Î»)y) â‰¤ Î»f(x) + (1-Î»)f(y)
```
for all x, y in domain and Î» âˆˆ [0,1]

#### Properties:
- Local minimum is global minimum
- Gradient descent converges to global minimum
- Many ML problems are convex

#### Examples:
- Linear functions
- Quadratic functions (positive definite)
- Exponential functions
- Logarithmic functions (on positive domain)

### 5.3 Lagrange Multipliers ğŸ”´

For constrained optimization:
```
L(x,Î») = f(x) + Î£Î»igi(x)
```

#### Necessary Conditions (KKT):
```
âˆ‡f(x*) + Î£Î»iâˆ‡gi(x*) = 0
gi(x*) â‰¤ 0
Î»i â‰¥ 0
Î»igi(x*) = 0
```

#### ML Applications:
- **Support Vector Machines**
- **Constrained optimization in neural networks**

---

## 6. Numerical Methods

Practical computation methods for implementing ML algorithms.

### 6.1 Root Finding ğŸŸ¡

#### Newton's Method:
```
xn+1 = xn - f(xn)/f'(xn)
```

#### Bisection Method:
Repeatedly halve interval containing root.

### 6.2 Numerical Integration ğŸŸ¡

#### Trapezoidal Rule:
```
âˆ«[a to b] f(x)dx â‰ˆ (b-a)/2n Ã— Î£[f(xi) + f(xi+1)]
```

#### Monte Carlo Integration:
Use random sampling to estimate integrals.

### 6.3 Linear System Solving ğŸŸ¡

#### Gaussian Elimination:
Systematic method to solve Ax = b.

#### LU Decomposition:
```
A = LU
```
Where L is lower triangular, U is upper triangular.

#### Iterative Methods:
- **Jacobi method**
- **Gauss-Seidel method**
- **Conjugate gradient**

---

## 7. Practical Applications in ML

### 7.1 Linear Regression Mathematics ğŸŸ¢

#### Matrix Form:
```
y = XÎ² + Îµ
```

#### Normal Equation:
```
Î²Ì‚ = (Xáµ€X)â»Â¹Xáµ€y
```

#### Cost Function:
```
J(Î²) = (1/2m)||XÎ² - y||Â²
```

#### Gradient:
```
âˆ‡J(Î²) = (1/m)Xáµ€(XÎ² - y)
```

### 7.2 Logistic Regression Mathematics ğŸŸ¡

#### Sigmoid Function:
```
Ïƒ(z) = 1/(1 + eâ»á¶»)
```

#### Prediction:
```
P(y=1|x) = Ïƒ(Î²áµ€x)
```

#### Log-Likelihood:
```
â„“(Î²) = Î£[yi log Ïƒ(Î²áµ€xi) + (1-yi) log(1-Ïƒ(Î²áµ€xi))]
```

#### Gradient:
```
âˆ‡â„“(Î²) = Î£(yi - Ïƒ(Î²áµ€xi))xi
```

### 7.3 Neural Network Mathematics ğŸŸ¡

#### Forward Propagation:
```
zâ½Ë¡â¾ = Wâ½Ë¡â¾aâ½Ë¡â»Â¹â¾ + bâ½Ë¡â¾
aâ½Ë¡â¾ = Ïƒ(zâ½Ë¡â¾)
```

#### Backpropagation:
```
Î´â½Ë¡â¾ = (Wâ½Ë¡âºÂ¹â¾)áµ€Î´â½Ë¡âºÂ¹â¾ âŠ™ Ïƒ'(zâ½Ë¡â¾)
```

#### Parameter Updates:
```
Wâ½Ë¡â¾ := Wâ½Ë¡â¾ - Î± Ã— Î´â½Ë¡âºÂ¹â¾(aâ½Ë¡â¾)áµ€
bâ½Ë¡â¾ := bâ½Ë¡â¾ - Î± Ã— Î´â½Ë¡âºÂ¹â¾
```

---

## ğŸ¯ Key Mathematical Concepts Summary

### Essential for ML Success:
1. **Linear Algebra**: Matrix operations, eigenvalues, SVD
2. **Calculus**: Derivatives, gradients, optimization
3. **Probability**: Distributions, Bayes' theorem, expectation
4. **Statistics**: Hypothesis testing, correlation, regression
5. **Optimization**: Gradient descent, convex optimization

### Common Patterns:
- **Data as matrices**: Rows = samples, columns = features
- **Optimization**: Most ML = optimization problem
- **Probability**: Handle uncertainty and make predictions
- **Linear algebra**: Efficient computation with vectors/matrices

---

## ğŸ“š Next Steps

Continue your journey with:
- **[Data Preprocessing](03_Data_Preprocessing.md)** - Applying math to prepare real data
- **[Supervised Learning](04_Supervised_Learning.md)** - See math in action with algorithms

---

## ğŸ”¢ Practice Problems

### Problem 1: Matrix Operations
Given matrices:
```
A = [1  2]    B = [5  6]
    [3  4]        [7  8]
```
Calculate: A + B, AB, Aáµ€, and det(A)

### Problem 2: Gradient Calculation
For f(x,y) = xÂ² + 2xy + yÂ², find:
- âˆ‡f
- Critical points
- Hessian matrix

### Problem 3: Probability
If P(Disease) = 0.01 and a test has:
- P(Positive|Disease) = 0.95
- P(Positive|No Disease) = 0.05

Find P(Disease|Positive) using Bayes' theorem.

---

*Next: [Data Preprocessing â†’](03_Data_Preprocessing.md)*
